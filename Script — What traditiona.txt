Problem Statement — What traditional systems don’t do (and what we did)
Most e‑commerce stacks still treat reviews as opaque text blobs or simple aggregates (averages, counts). That approach blocks end‑to‑end intelligence because it (1) misses structured extraction of actionable units, (2) cannot trace or audit AI decisions, (3) fails to serve persona‑specific decision points, and (4) depends on third‑party inference, inflating latency/cost and risking data governance.
Where legacy breaks down (technically):
Unstructured outputs: Keyword lists and star means are not a contract; you can’t reliably pipe them into downstream analytics without brittle regex or ad‑hoc ETL. There’s no standard payload for pros/cons, verdict, sentiment, key issues, or FAQs, so BI layers struggle to materialize KPIs beyond averages. [Sprint Documentation | PDF]
No AI traceability: Typical systems don’t store raw LLM output, prompt version, model/provider, tokens in/out, or latency next to relational facts. That kills reproducibility and regression debugging—you can’t answer “why did the summary change?” or “which prompt generated this verdict?” at audit time. [Sprint Documentation | PDF]
Persona‑agnostic UX: Buyers, sellers, support, and managers need different signals—buyers want concise summaries/FAQs; sellers want pros/cons top‑N and complaint distributions; support needs severe review triage with sparklines; managers need trend lines, donut distributions, and mismatch heuristics (rating vs. text). Legacy UIs rarely deliver per‑role metrics & escalation paths. [Sprint Documentation | PDF]
Externalized inference: Cloud‑only LLM calls add per‑request cost, data egress risk, and latency variability. Without PEFT/LoRA adapters on local GPU, teams can’t control TCO or enforce zero external dependency for sensitive review data. [Sprint Documentation | PDF]
What we engineered (end‑to‑end, audit‑first):
A contracted pipeline: Datasets → preprocessing → LoRA‑fine‑tuned DeepSeek → JSON outputs → PostgreSQL JSONB + pgvector → FastAPI contracts → React SPA. Each review yields one LLM analysis row with summary, pros, cons, sentiment, verdict, key themes, plus raw_output, prompt_version, llm_model/provider, tokens_input/output, latency_ms for full traceability. (…[PostgreSQL: analyses, product_analyses, product_questions, product_answers, product_embeddings]) [data-1767592899860 | Excel], [Sprint Documentation | PDF]
Local GPU inference (PEFT/LoRA): We load DeepSeek‑1.3B‑Instruct and inject adapters for task‑specific instruction following; outputs respect a fixed JSON schema that downstream services and dashboards consume deterministically. (…[LLM Colab notebook; PEFT adapter load]) [Sprint Documentation | PDF]
Role APIs and contracts: Buyer gets /buyer/product/:id/summary; Seller gets /seller/product/:id/insights; Support gets /support/product/:id/urgent; Manager gets /dashboards/product/:id/sentiment-trend and /dashboards/product/:id/key-issues. These FastAPI contracts standardize payloads for each persona and enable non‑breaking frontend rendering. (…[FastAPI: /buyer/product/:id/summary, /dashboards/, /support/]) [Sprint Documentation | PDF]
Persona‑aware React SPA: Customer.jsx, ProductDetails.jsx, Seller.jsx, Support.jsx, Manager.jsx render audit‑ready insights with pure SVG charts and defensive fetch/normalization, including FAQ shape reconciliation and image fallbacks. (…[React pages: Customer.jsx, ProductDetails.jsx, Seller.jsx, Support.jsx, Manager.jsx]) [Sprint Documentation | PDF]
Result: We moved from static text and averages to a governed intelligence fabric: structured review analytics, provable AI lineage, persona‑targeted KPIs, and cost‑controlled local inference, all backed by PostgreSQL JSONB/pgvector and FastAPI schemas that the React SPA consumes without brittle coupling. (…[Sprint Documentation | PDF])
 
get slide content from copilot, and paste it as bullets on our ppt
 
2) Abstract (expanded)
The Review Intelligence System (RIS) operates as a contract‑driven analytics pipeline that begins with e‑commerce datasets—products, reviews, and Q\&A—and terminates in persona‑specific decision surfaces. Ingested data is normalized for relational integrity and LLM readiness, then passed through a local GPU inference layer powered by PEFT/LoRA adapters on DeepSeek Coder‑1.3B Instruct to produce strict, machine‑readable JSON: summary, pros[], cons[], sentiment, verdict, and key_issues[]. The system enforces schema determinism through instruction prompts and validates outputs before persisting them into PostgreSQL JSONB, co‑storing traceability fields—raw_output, prompt_version, llm_model, provider, tokens_input/output, latency_ms—to guarantee auditability and reproducibility across releases (…[LLM Colab notebook; PEFT/LoRA adapter upload; PostgreSQL JSONB]).
A service aggregation layer composes materialized views and role APIs: buyers receive product‑level summaries with pros/cons and avg rating; sellers get insights (sentiment distribution, trend points, complaint tallies); support teams receive severity feeds and sparklines for urgent triage; managers consume KPI cards, line/donut charts, and a mismatch heuristic (rating vs. text) for quality risk tracking (…[FastAPI endpoints: /buyer/product/:id/summary, /seller/product/:id/insights, /dashboards/product/:id/sentiment-trend, /dashboards/product/:id/key-issues, /support/product/:id/urgent]).
The React SPA renders these contracts defensively: pages use inline light‑theme overrides, pure SVG charting for deterministic rendering, and FAQ normalization to absorb backend shape variations without runtime breakage. Buyer‑facing pages show succinct summaries and FAQ toggles; Seller surfaces visualize pros/cons top‑N and trendlines; Support consoles highlight severe review chips, progress bars, and urgent tables; Manager consoles present KPI, donut distributions, and line trends with legends. The application favors non‑blocking fetch patterns, error fallbacks (image and payload), and role‑aware navigation that aligns with JWT claims issued by the backend (…[React role pages: Customer.jsx, ProductDetails.jsx, Seller.jsx, Support.jsx, Manager.jsx; FastAPI endpoints]).
Overall, RIS replaces ad‑hoc text mining with a governed intelligence substrate: curated instruction prompts, LoRA‑adapted inference under full cost/control, contract‑validated JSON outputs with telemetry, and persona APIs that separate concerns between content extraction and decision rendering—yielding a composable, enterprise‑ready analytics stack (…[LLM Colab notebook; FastAPI endpoints; PostgreSQL JSONB; React role pages]).
3) Preparing instruction‑style datasets (LLM): products, reviews, product‑answers, product‑questions (expanded)
Data sources & schema contracts
RIS stages canonical product master and review facts, plus Q\&A pairs for downstream prompt conditioning and RAG‑style augmentation.
Products master: minimal, stable identifiers and attributes—product_id, product_title, brand, category, subtype, price, sku/asin, and a seller link; grain is one product per row (…[products‑2000.csv]).
Reviews: review_id, FK product_id, rating, review_title, review_text, review_date, reviewer_name, verified_purchase, helpful_votes, language; dates/language normalized for time‑series and prompt localization (…[reviews‑10000.csv]).
Product Q\&A: product_id × question_text composite key with guaranteed 1:1 answer integrity; stores answer_text and timestamps for recency preference (…[product‑questions‑8000.csv], [product‑answers‑8000.csv]).
Schema dictionary: authoritative listing of domain tables (e.g., analyses, product_analyses, product_questions, product_answers, product_embeddings, reviews, products, sellers, support_teams, users) and staging assets, used to verify types, keys, and relationships (…[data-1767592899860.csv], [Sprint Documentation | PDF]).
Instruction formatting (LLM)
Each review row is transformed into an instruction‑style prompt consisting of:
Task header (explicit extraction contract),
Input block (verbatim review text, optionally with product metadata for disambiguation), and
Response scaffold (fixed JSON schema to enforce field presence and types).
This scaffold ensures the decoder emits a deterministic structure at generation time—summary, pros[], cons[], sentiment, verdict, key_issues[]—reducing post‑processing overhead and eliminating fragile regex pipelines. Tokenization uses right‑padding and length caps to stabilize batch collation; labels mirror input ids for causal LM training (next‑token prediction), which makes the model learn both format and content fidelity in a single objective (…[LLM Colab notebook cells: prompt builder; tokenizer; prepare labels]).
LLM‑ready normalization
Before prompt construction, reviews pass through text preprocessing (whitespace, punctuation, Unicode normalization), date canonicalization, language codes, and verified_purchase coercion. For Q\&A, deduplication on (product_id, question_text) and optional recency scoring allow instruction priming (e.g., appending the most recent authoritative answer for context). The final prepared batches—products for grounding, reviews for extraction, and Q\&A for contextual hints—feed the LoRA training and later inference paths with consistent, contractable inputs (…[products‑2000.csv], [reviews‑10000.csv], [product‑questions‑8000.csv], [product‑answers‑8000.csv], [LLM Colab notebook cells]).
 
2) Abstract (expanded)
The Review Intelligence System (RIS) operates as a contract‑driven analytics pipeline that begins with e‑commerce datasets—products, reviews, and Q\&A—and terminates in persona‑specific decision surfaces. Ingested data is normalized for relational integrity and LLM readiness, then passed through a local GPU inference layer powered by PEFT/LoRA adapters on DeepSeek Coder‑1.3B Instruct to produce strict, machine‑readable JSON: summary, pros[], cons[], sentiment, verdict, and key_issues[]. The system enforces schema determinism through instruction prompts and validates outputs before persisting them into PostgreSQL JSONB, co‑storing traceability fields—raw_output, prompt_version, llm_model, provider, tokens_input/output, latency_ms—to guarantee auditability and reproducibility across releases (…[LLM Colab notebook; PEFT/LoRA adapter upload; PostgreSQL JSONB]).
A service aggregation layer composes materialized views and role APIs: buyers receive product‑level summaries with pros/cons and avg rating; sellers get insights (sentiment distribution, trend points, complaint tallies); support teams receive severity feeds and sparklines for urgent triage; managers consume KPI cards, line/donut charts, and a mismatch heuristic (rating vs. text) for quality risk tracking (…[FastAPI endpoints: /buyer/product/:id/summary, /seller/product/:id/insights, /dashboards/product/:id/sentiment-trend, /dashboards/product/:id/key-issues, /support/product/:id/urgent]).
The React SPA renders these contracts defensively: pages use inline light‑theme overrides, pure SVG charting for deterministic rendering, and FAQ normalization to absorb backend shape variations without runtime breakage. Buyer‑facing pages show succinct summaries and FAQ toggles; Seller surfaces visualize pros/cons top‑N and trendlines; Support consoles highlight severe review chips, progress bars, and urgent tables; Manager consoles present KPI, donut distributions, and line trends with legends. The application favors non‑blocking fetch patterns, error fallbacks (image and payload), and role‑aware navigation that aligns with JWT claims issued by the backend (…[React role pages: Customer.jsx, ProductDetails.jsx, Seller.jsx, Support.jsx, Manager.jsx; FastAPI endpoints]).
Overall, RIS replaces ad‑hoc text mining with a governed intelligence substrate: curated instruction prompts, LoRA‑adapted inference under full cost/control, contract‑validated JSON outputs with telemetry, and persona APIs that separate concerns between content extraction and decision rendering—yielding a composable, enterprise‑ready analytics stack (…[LLM Colab notebook; FastAPI endpoints; PostgreSQL JSONB; React role pages]).
3) Preparing instruction‑style datasets (LLM): products, reviews, product‑answers, product‑questions (expanded)
Data sources & schema contracts
RIS stages canonical product master and review facts, plus Q\&A pairs for downstream prompt conditioning and RAG‑style augmentation.
Products master: minimal, stable identifiers and attributes—product_id, product_title, brand, category, subtype, price, sku/asin, and a seller link; grain is one product per row (…[products‑2000.csv]).
Reviews: review_id, FK product_id, rating, review_title, review_text, review_date, reviewer_name, verified_purchase, helpful_votes, language; dates/language normalized for time‑series and prompt localization (…[reviews‑10000.csv]).
Product Q\&A: product_id × question_text composite key with guaranteed 1:1 answer integrity; stores answer_text and timestamps for recency preference (…[product‑questions‑8000.csv], [product‑answers‑8000.csv]).
Schema dictionary: authoritative listing of domain tables (e.g., analyses, product_analyses, product_questions, product_answers, product_embeddings, reviews, products, sellers, support_teams, users) and staging assets, used to verify types, keys, and relationships (…[data-1767592899860.csv], [Sprint Documentation | PDF]).
Instruction formatting (LLM)
Each review row is transformed into an instruction‑style prompt consisting of:
Task header (explicit extraction contract),
Input block (verbatim review text, optionally with product metadata for disambiguation), and
Response scaffold (fixed JSON schema to enforce field presence and types).
This scaffold ensures the decoder emits a deterministic structure at generation time—summary, pros[], cons[], sentiment, verdict, key_issues[]—reducing post‑processing overhead and eliminating fragile regex pipelines. Tokenization uses right‑padding and length caps to stabilize batch collation; labels mirror input ids for causal LM training (next‑token prediction), which makes the model learn both format and content fidelity in a single objective (…[LLM Colab notebook cells: prompt builder; tokenizer; prepare labels]).
LLM‑ready normalization
Before prompt construction, reviews pass through text preprocessing (whitespace, punctuation, Unicode normalization), date canonicalization, language codes, and verified_purchase coercion. For Q\&A, deduplication on (product_id, question_text) and optional recency scoring allow instruction priming (e.g., appending the most recent authoritative answer for context). The final prepared batches—products for grounding, reviews for extraction, and Q\&A for contextual hints—feed the LoRA training and later inference paths with consistent, contractable inputs (…[products‑2000.csv], [reviews‑10000.csv], [product‑questions‑8000.csv], [product‑answers‑8000.csv], [LLM Colab notebook cells]).
 

Problem Statement — What traditional systems don’t do (and what we did)
Most e‑commerce stacks still treat reviews as opaque text blobs or simple aggregates (averages, counts). That approach blocks end‑to‑end intelligence because it (1) misses structured extraction of actionable units, (2) cannot trace or audit AI decisions, (3) fails to serve persona‑specific decision points, and (4) depends on third‑party inference, inflating latency/cost and risking data governance.
Where legacy breaks down (technically):
Unstructured outputs: Keyword lists and star means are not a contract; you can’t reliably pipe them into downstream analytics without brittle regex or ad‑hoc ETL. There’s no standard payload for pros/cons, verdict, sentiment, key issues, or FAQs, so BI layers struggle to materialize KPIs beyond averages. [Sprint Documentation | PDF]
No AI traceability: Typical systems don’t store raw LLM output, prompt version, model/provider, tokens in/out, or latency next to relational facts. That kills reproducibility and regression debugging—you can’t answer “why did the summary change?” or “which prompt generated this verdict?” at audit time. [Sprint Documentation | PDF]
Persona‑agnostic UX: Buyers, sellers, support, and managers need different signals—buyers want concise summaries/FAQs; sellers want pros/cons top‑N and complaint distributions; support needs severe review triage with sparklines; managers need trend lines, donut distributions, and mismatch heuristics (rating vs. text). Legacy UIs rarely deliver per‑role metrics & escalation paths. [Sprint Documentation | PDF]
Externalized inference: Cloud‑only LLM calls add per‑request cost, data egress risk, and latency variability. Without PEFT/LoRA adapters on local GPU, teams can’t control TCO or enforce zero external dependency for sensitive review data. [Sprint Documentation | PDF]
What we engineered (end‑to‑end, audit‑first):
A contracted pipeline: Datasets → preprocessing → LoRA‑fine‑tuned DeepSeek → JSON outputs → PostgreSQL JSONB + pgvector → FastAPI contracts → React SPA. Each review yields one LLM analysis row with summary, pros, cons, sentiment, verdict, key themes, plus raw_output, prompt_version, llm_model/provider, tokens_input/output, latency_ms for full traceability. (…[PostgreSQL: analyses, product_analyses, product_questions, product_answers, product_embeddings]) [data-1767592899860 | Excel], [Sprint Documentation | PDF]
Local GPU inference (PEFT/LoRA): We load DeepSeek‑1.3B‑Instruct and inject adapters for task‑specific instruction following; outputs respect a fixed JSON schema that downstream services and dashboards consume deterministically. (…[LLM Colab notebook; PEFT adapter load]) [Sprint Documentation | PDF]
Role APIs and contracts: Buyer gets /buyer/product/:id/summary; Seller gets /seller/product/:id/insights; Support gets /support/product/:id/urgent; Manager gets /dashboards/product/:id/sentiment-trend and /dashboards/product/:id/key-issues. These FastAPI contracts standardize payloads for each persona and enable non‑breaking frontend rendering. (…[FastAPI: /buyer/product/:id/summary, /dashboards/, /support/]) [Sprint Documentation | PDF]
Persona‑aware React SPA: Customer.jsx, ProductDetails.jsx, Seller.jsx, Support.jsx, Manager.jsx render audit‑ready insights with pure SVG charts and defensive fetch/normalization, including FAQ shape reconciliation and image fallbacks. (…[React pages: Customer.jsx, ProductDetails.jsx, Seller.jsx, Support.jsx, Manager.jsx]) [Sprint Documentation | PDF]
Result: We moved from static text and averages to a governed intelligence fabric: structured review analytics, provable AI lineage, persona‑targeted KPIs, and cost‑controlled local inference, all backed by PostgreSQL JSONB/pgvector and FastAPI schemas that the React SPA consumes without brittle coupling. (…[Sprint Documentation | PDF])
 
get slide content from copilot, and paste it as bullets on our ppt
 
2) Abstract (expanded)
The Review Intelligence System (RIS) operates as a contract‑driven analytics pipeline that begins with e‑commerce datasets—products, reviews, and Q\&A—and terminates in persona‑specific decision surfaces. Ingested data is normalized for relational integrity and LLM readiness, then passed through a local GPU inference layer powered by PEFT/LoRA adapters on DeepSeek Coder‑1.3B Instruct to produce strict, machine‑readable JSON: summary, pros[], cons[], sentiment, verdict, and key_issues[]. The system enforces schema determinism through instruction prompts and validates outputs before persisting them into PostgreSQL JSONB, co‑storing traceability fields—raw_output, prompt_version, llm_model, provider, tokens_input/output, latency_ms—to guarantee auditability and reproducibility across releases (…[LLM Colab notebook; PEFT/LoRA adapter upload; PostgreSQL JSONB]).
A service aggregation layer composes materialized views and role APIs: buyers receive product‑level summaries with pros/cons and avg rating; sellers get insights (sentiment distribution, trend points, complaint tallies); support teams receive severity feeds and sparklines for urgent triage; managers consume KPI cards, line/donut charts, and a mismatch heuristic (rating vs. text) for quality risk tracking (…[FastAPI endpoints: /buyer/product/:id/summary, /seller/product/:id/insights, /dashboards/product/:id/sentiment-trend, /dashboards/product/:id/key-issues, /support/product/:id/urgent]).
The React SPA renders these contracts defensively: pages use inline light‑theme overrides, pure SVG charting for deterministic rendering, and FAQ normalization to absorb backend shape variations without runtime breakage. Buyer‑facing pages show succinct summaries and FAQ toggles; Seller surfaces visualize pros/cons top‑N and trendlines; Support consoles highlight severe review chips, progress bars, and urgent tables; Manager consoles present KPI, donut distributions, and line trends with legends. The application favors non‑blocking fetch patterns, error fallbacks (image and payload), and role‑aware navigation that aligns with JWT claims issued by the backend (…[React role pages: Customer.jsx, ProductDetails.jsx, Seller.jsx, Support.jsx, Manager.jsx; FastAPI endpoints]).
Overall, RIS replaces ad‑hoc text mining with a governed intelligence substrate: curated instruction prompts, LoRA‑adapted inference under full cost/control, contract‑validated JSON outputs with telemetry, and persona APIs that separate concerns between content extraction and decision rendering—yielding a composable, enterprise‑ready analytics stack (…[LLM Colab notebook; FastAPI endpoints; PostgreSQL JSONB; React role pages]).
3) Preparing instruction‑style datasets (LLM): products, reviews, product‑answers, product‑questions (expanded)
Data sources & schema contracts
RIS stages canonical product master and review facts, plus Q\&A pairs for downstream prompt conditioning and RAG‑style augmentation.
Products master: minimal, stable identifiers and attributes—product_id, product_title, brand, category, subtype, price, sku/asin, and a seller link; grain is one product per row (…[products‑2000.csv]).
Reviews: review_id, FK product_id, rating, review_title, review_text, review_date, reviewer_name, verified_purchase, helpful_votes, language; dates/language normalized for time‑series and prompt localization (…[reviews‑10000.csv]).
Product Q\&A: product_id × question_text composite key with guaranteed 1:1 answer integrity; stores answer_text and timestamps for recency preference (…[product‑questions‑8000.csv], [product‑answers‑8000.csv]).
Schema dictionary: authoritative listing of domain tables (e.g., analyses, product_analyses, product_questions, product_answers, product_embeddings, reviews, products, sellers, support_teams, users) and staging assets, used to verify types, keys, and relationships (…[data-1767592899860.csv], [Sprint Documentation | PDF]).
Instruction formatting (LLM)
Each review row is transformed into an instruction‑style prompt consisting of:
Task header (explicit extraction contract),
Input block (verbatim review text, optionally with product metadata for disambiguation), and
Response scaffold (fixed JSON schema to enforce field presence and types).
This scaffold ensures the decoder emits a deterministic structure at generation time—summary, pros[], cons[], sentiment, verdict, key_issues[]—reducing post‑processing overhead and eliminating fragile regex pipelines. Tokenization uses right‑padding and length caps to stabilize batch collation; labels mirror input ids for causal LM training (next‑token prediction), which makes the model learn both format and content fidelity in a single objective (…[LLM Colab notebook cells: prompt builder; tokenizer; prepare labels]).
LLM‑ready normalization
Before prompt construction, reviews pass through text preprocessing (whitespace, punctuation, Unicode normalization), date canonicalization, language codes, and verified_purchase coercion. For Q\&A, deduplication on (product_id, question_text) and optional recency scoring allow instruction priming (e.g., appending the most recent authoritative answer for context). The final prepared batches—products for grounding, reviews for extraction, and Q\&A for contextual hints—feed the LoRA training and later inference paths with consistent, contractable inputs (…[products‑2000.csv], [reviews‑10000.csv], [product‑questions‑8000.csv], [product‑answers‑8000.csv], [LLM Colab notebook cells]).
 
2) Abstract (expanded)
The Review Intelligence System (RIS) operates as a contract‑driven analytics pipeline that begins with e‑commerce datasets—products, reviews, and Q\&A—and terminates in persona‑specific decision surfaces. Ingested data is normalized for relational integrity and LLM readiness, then passed through a local GPU inference layer powered by PEFT/LoRA adapters on DeepSeek Coder‑1.3B Instruct to produce strict, machine‑readable JSON: summary, pros[], cons[], sentiment, verdict, and key_issues[]. The system enforces schema determinism through instruction prompts and validates outputs before persisting them into PostgreSQL JSONB, co‑storing traceability fields—raw_output, prompt_version, llm_model, provider, tokens_input/output, latency_ms—to guarantee auditability and reproducibility across releases (…[LLM Colab notebook; PEFT/LoRA adapter upload; PostgreSQL JSONB]).
A service aggregation layer composes materialized views and role APIs: buyers receive product‑level summaries with pros/cons and avg rating; sellers get insights (sentiment distribution, trend points, complaint tallies); support teams receive severity feeds and sparklines for urgent triage; managers consume KPI cards, line/donut charts, and a mismatch heuristic (rating vs. text) for quality risk tracking (…[FastAPI endpoints: /buyer/product/:id/summary, /seller/product/:id/insights, /dashboards/product/:id/sentiment-trend, /dashboards/product/:id/key-issues, /support/product/:id/urgent]).
The React SPA renders these contracts defensively: pages use inline light‑theme overrides, pure SVG charting for deterministic rendering, and FAQ normalization to absorb backend shape variations without runtime breakage. Buyer‑facing pages show succinct summaries and FAQ toggles; Seller surfaces visualize pros/cons top‑N and trendlines; Support consoles highlight severe review chips, progress bars, and urgent tables; Manager consoles present KPI, donut distributions, and line trends with legends. The application favors non‑blocking fetch patterns, error fallbacks (image and payload), and role‑aware navigation that aligns with JWT claims issued by the backend (…[React role pages: Customer.jsx, ProductDetails.jsx, Seller.jsx, Support.jsx, Manager.jsx; FastAPI endpoints]).
Overall, RIS replaces ad‑hoc text mining with a governed intelligence substrate: curated instruction prompts, LoRA‑adapted inference under full cost/control, contract‑validated JSON outputs with telemetry, and persona APIs that separate concerns between content extraction and decision rendering—yielding a composable, enterprise‑ready analytics stack (…[LLM Colab notebook; FastAPI endpoints; PostgreSQL JSONB; React role pages]).
3) Preparing instruction‑style datasets (LLM): products, reviews, product‑answers, product‑questions (expanded)
Data sources & schema contracts
RIS stages canonical product master and review facts, plus Q\&A pairs for downstream prompt conditioning and RAG‑style augmentation.
Products master: minimal, stable identifiers and attributes—product_id, product_title, brand, category, subtype, price, sku/asin, and a seller link; grain is one product per row (…[products‑2000.csv]).
Reviews: review_id, FK product_id, rating, review_title, review_text, review_date, reviewer_name, verified_purchase, helpful_votes, language; dates/language normalized for time‑series and prompt localization (…[reviews‑10000.csv]).
Product Q\&A: product_id × question_text composite key with guaranteed 1:1 answer integrity; stores answer_text and timestamps for recency preference (…[product‑questions‑8000.csv], [product‑answers‑8000.csv]).
Schema dictionary: authoritative listing of domain tables (e.g., analyses, product_analyses, product_questions, product_answers, product_embeddings, reviews, products, sellers, support_teams, users) and staging assets, used to verify types, keys, and relationships (…[data-1767592899860.csv], [Sprint Documentation | PDF]).
Instruction formatting (LLM)
Each review row is transformed into an instruction‑style prompt consisting of:
Task header (explicit extraction contract),
Input block (verbatim review text, optionally with product metadata for disambiguation), and
Response scaffold (fixed JSON schema to enforce field presence and types).
This scaffold ensures the decoder emits a deterministic structure at generation time—summary, pros[], cons[], sentiment, verdict, key_issues[]—reducing post‑processing overhead and eliminating fragile regex pipelines. Tokenization uses right‑padding and length caps to stabilize batch collation; labels mirror input ids for causal LM training (next‑token prediction), which makes the model learn both format and content fidelity in a single objective (…[LLM Colab notebook cells: prompt builder; tokenizer; prepare labels]).
LLM‑ready normalization
Before prompt construction, reviews pass through text preprocessing (whitespace, punctuation, Unicode normalization), date canonicalization, language codes, and verified_purchase coercion. For Q\&A, deduplication on (product_id, question_text) and optional recency scoring allow instruction priming (e.g., appending the most recent authoritative answer for context). The final prepared batches—products for grounding, reviews for extraction, and Q\&A for contextual hints—feed the LoRA training and later inference paths with consistent, contractable inputs (…[products‑2000.csv], [reviews‑10000.csv], [product‑questions‑8000.csv], [product‑answers‑8000.csv], [LLM Colab notebook cells]).
 